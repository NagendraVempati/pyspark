pyspark.SparkContext is an entry point to the PySpark functionality that is used to communicate with the cluster and to create an RDD, accumulator, and broadcast variables.  Note that you can create only one SparkContext per JVM, in order to create another first 
you need to stop the existing one using stop() method.

The Spark driver program creates and uses SparkContext to connect to the cluster manager to submit PySpark jobs, and know what resource manager (YARN, Mesos, or Standalone) to communicate to. It is the heart of the PySpark application.
By default, PySpark shell creates and provides sc object, which is an instance of the SparkContext class. We can directly use this object where required without the need to create.

Since PySpark 2.0, Creating a SparkSession creates a SparkContext internally and exposes the sparkContext variable to use.At any given time only one SparkContext instance should be active per JVM. In case you want to create another you should stop existing SparkContext using stop() before creating a new one. -- spark.sparkContext.stop()
you can create any number of SparkSession objects however, for all those objects underlying there will be only one SparkContext.

You can create SparkContext by programmatically using its constructor, and pass parameters like master and appName at least as these are mandatory params. The below example creates context with a master as local and app name as Spark_Example_App.
--*
from pyspark import SparkContext
sc = SparkContext("local", "Spark_Example_App")
print(sc.appName)

You can also create it using SparkContext.getOrCreate(). 
--*
# Create Spark Context
from pyspark import SparkConf, SparkContext
conf = SparkConf()
conf.setMaster("local").setAppName("Spark Example App")
sc = SparkContext.getOrCreate(conf)
print(sc.appName)

SparkContext Commonly Used Methods
accumulator(value[, accum_param]) – It creates an pyspark accumulator variable with initial specified value. Only a driver can access accumulator variables.
broadcast(value) – read-only PySpark broadcast variable. This will be broadcast to the entire cluster. You can broadcast a variable to a PySpark cluster only once.
-----------------------------------------------------------------------------------------------------------------------------------------------------------
In real-time production environments, Spark doesn't run on your local machine. Instead, it's executed on a cluster (like YARN, Kubernetes, or a Databricks-managed environment). So, the way we create the SparkContext is different — you don’t explicitly create it most of the time.
When submitting your Spark job to a YARN cluster:
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  my_script.py
-----------------------------------
In your code:
from pyspark import SparkContext
sc = SparkContext(appName="MyYarnApp")

--master yarn tells Spark to use the YARN cluster manager
No need to pass "local" in SparkContext — the spark-submit sets it.

Kubernetes or EMR Cluster
spark-submit --master k8s://<k8s-cluster-url> ... (or)
spark-submit --master yarn ...(or)
sc = SparkContext(appName="MyK8sApp")

finally ..
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("MyApp").getOrCreate()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

In local mode, there is no external resource manager like YARN, Mesos, or Kubernetes.
Instead, your local JVM process (your machine itself) acts as the resource manager.

You're telling Spark:
“Run everything — the driver and executors — on this machine using a single JVM process (or multiple threads if configured).”
So, Spark manages resources using your local OS thread scheduling and memory. There's no central resource manager involved.
if its a local what is the resource manager
